"""
Webé¡µé¢åˆ†æ - é¡µé¢æˆªå›¾æ™ºèƒ½åˆ†æAPI
æ”¯æŒSSEæµå¼æ¥å£å’Œé¡µé¢å…ƒç´ è¯†åˆ«
"""
from autogen_core import CancellationToken, MessageContext, ClosureContext
from fastapi import APIRouter, Request, Depends, HTTPException, BackgroundTasks, File, UploadFile, Form
from fastapi.responses import JSONResponse, FileResponse
from sse_starlette.sse import EventSourceResponse
import asyncio
import logging
import uuid
import json
import base64
import time
import os
import aiofiles
from typing import Dict, List, Optional, Any
from datetime import datetime
from pathlib import Path
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.messages import StreamMessage
from app.core.messages.web import WebMultimodalAnalysisRequest
from app.database.connection import db_manager
from app.database.repositories.page_analysis_repository import PageAnalysisRepository, PageElementRepository

router = APIRouter()

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# ä¼šè¯å­˜å‚¨
active_sessions: Dict[str, Dict[str, Any]] = {}

# æ¶ˆæ¯é˜Ÿåˆ—å­˜å‚¨
message_queues: Dict[str, asyncio.Queue] = {}

# ä¼šè¯è¶…æ—¶ï¼ˆç§’ï¼‰
SESSION_TIMEOUT = 3600  # 1å°æ—¶

# å›¾ç‰‡å­˜å‚¨é…ç½®
UPLOAD_DIR = Path("uploads/images")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

async def save_uploaded_image(file: UploadFile, session_id: str, file_index: int) -> tuple[str, str]:
    """
    ä¿å­˜ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶

    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
        session_id: ä¼šè¯ID
        file_index: æ–‡ä»¶ç´¢å¼•

    Returns:
        tuple: (æ–‡ä»¶è·¯å¾„, åŸå§‹æ–‡ä»¶å)
    """
    try:
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        file_extension = Path(file.filename).suffix if file.filename else '.png'
        unique_filename = f"{session_id}_{file_index}_{int(time.time())}{file_extension}"
        file_path = UPLOAD_DIR / unique_filename

        # ä¿å­˜æ–‡ä»¶
        content = await file.read()
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(content)

        logger.info(f"å›¾ç‰‡å·²ä¿å­˜: {file_path}")
        return str(file_path), file.filename or f"image_{file_index}{file_extension}"

    except Exception as e:
        logger.error(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {str(e)}")


async def get_db_session():
    """è·å–æ•°æ®åº“ä¼šè¯"""
    try:
        logger.info("æ­£åœ¨è·å–æ•°æ®åº“ä¼šè¯...")
        async with db_manager.get_session() as session:
            logger.info("æ•°æ®åº“ä¼šè¯è·å–æˆåŠŸ")
            yield session
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“ä¼šè¯å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise


async def cleanup_session(session_id: str, delay: int = SESSION_TIMEOUT):
    """åœ¨æŒ‡å®šå»¶è¿Ÿåæ¸…ç†ä¼šè¯èµ„æº"""
    await asyncio.sleep(delay)
    if session_id in active_sessions:
        logger.info(f"æ¸…ç†è¿‡æœŸä¼šè¯: {session_id}")
        active_sessions.pop(session_id, None)
        message_queues.pop(session_id, None)


@router.get("/health")
async def health_check():
    """é¡µé¢åˆ†æå¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "ok", "service": "web-page-analysis", "timestamp": datetime.now().isoformat()}


@router.post("/upload-and-analyze")
async def upload_and_analyze_pages(
    files: List[UploadFile] = File(...),
    description: Optional[str] = Form(None),
    page_url: Optional[str] = Form(None),
    page_name: Optional[str] = Form(None)
):
    """
    ä¸Šä¼ é¡µé¢æˆªå›¾å¹¶å¯åŠ¨AIåˆ†æä»»åŠ¡

    Args:
        files: ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
        description: é¡µé¢æè¿°
        page_url: é¡µé¢URLï¼ˆå¯é€‰ï¼‰
        page_name: é¡µé¢åç§°ï¼ˆå¯é€‰ï¼‰

    Returns:
        Dict: åŒ…å«session_idçš„å“åº”
    """
    try:
        # éªŒè¯æ–‡ä»¶
        if not files:
            raise HTTPException(status_code=400, detail="è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶")

        # ç”Ÿæˆä¼šè¯IDï¼ˆæå‰ç”Ÿæˆï¼Œç”¨äºæ–‡ä»¶å‘½åï¼‰
        session_id = str(uuid.uuid4())

        # éªŒè¯æ¯ä¸ªæ–‡ä»¶
        validated_files = []
        for file_index, file in enumerate(files):
            # éªŒè¯æ–‡ä»¶ç±»å‹
            if not file.content_type or not file.content_type.startswith('image/'):
                raise HTTPException(status_code=400, detail=f"æ–‡ä»¶ {file.filename} ä¸æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶")

            # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆ10MBé™åˆ¶ï¼‰
            content = await file.read()
            file_size = len(content)
            if file_size > 10 * 1024 * 1024:  # 10MB
                raise HTTPException(status_code=400, detail=f"å›¾ç‰‡æ–‡ä»¶ {file.filename} å¤§å°ä¸èƒ½è¶…è¿‡10MB")

            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆä»¥ä¾¿ä¿å­˜æ–‡ä»¶
            await file.seek(0)

            # ä¿å­˜å›¾ç‰‡æ–‡ä»¶
            image_path, original_filename = await save_uploaded_image(file, session_id, file_index)

            # è½¬æ¢ä¸ºbase64ï¼ˆç”¨äºåˆ†æï¼‰
            image_base64 = base64.b64encode(content).decode('utf-8')

            validated_files.append({
                "filename": original_filename,
                "content_type": file.content_type,
                "size": file_size,
                "image_data": image_base64,
                "image_path": image_path  # æ·»åŠ å›¾ç‰‡è·¯å¾„
            })

        # è®°å½•å½“å‰æ—¶é—´
        current_time = datetime.now()

        # ä¸è®¾ç½®é»˜è®¤å€¼ï¼Œä¿æŒç”¨æˆ·è¾“å…¥çš„åŸå§‹å€¼ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
        final_page_name = page_name.strip() if page_name else None
        final_description = description.strip() if description else None

        # å­˜å‚¨ä¼šè¯ä¿¡æ¯
        active_sessions[session_id] = {
            "status": "processing",  # ç›´æ¥è®¾ç½®ä¸ºå¤„ç†ä¸­
            "created_at": current_time.isoformat(),
            "last_activity": current_time.isoformat(),
            "files": validated_files,
            "page_info": {
                "description": final_description,
                "page_url": page_url.strip() if page_url else None,
                "page_name": final_page_name
            },
            "analysis_results": [],
            "progress": 0,
            "total_files": len(validated_files),
            "processed_files": 0
        }

        # åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—
        message_queue = asyncio.Queue()
        message_queues[session_id] = message_queue

        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºåˆå§‹æ•°æ®åº“è®°å½•ï¼ˆçŠ¶æ€ä¸ºprocessingï¼‰
        from app.database.connection import db_manager
        from app.database.models.page_analysis import PageAnalysisResult

        async with db_manager.get_session() as session:
            try:
                for i, file_info in enumerate(validated_files):
                    # ç”Ÿæˆåˆ†æID
                    analysis_id = str(uuid.uuid4())

                    # åˆ›å»ºåˆå§‹è®°å½•
                    page_analysis = PageAnalysisResult(
                        session_id=session_id,
                        analysis_id=analysis_id,
                        page_name=final_page_name or f"é¡µé¢åˆ†æ_{i+1}",
                        page_url=page_url.strip() if page_url else None,
                        page_description=final_description,
                        image_path=file_info.get('image_path'),  # æ·»åŠ å›¾ç‰‡è·¯å¾„
                        image_filename=file_info.get('filename'),  # æ·»åŠ åŸå§‹æ–‡ä»¶å
                        analysis_status='processing',  # è®¾ç½®ä¸ºå¤„ç†ä¸­çŠ¶æ€
                        confidence_score=0.0,
                        elements_count=0
                    )

                    session.add(page_analysis)

                await session.commit()
                logger.info(f"å·²åˆ›å»º {len(validated_files)} æ¡åˆå§‹åˆ†æè®°å½•ï¼ŒçŠ¶æ€ä¸ºprocessing")
            except Exception as e:
                await session.rollback()
                logger.error(f"åˆ›å»ºåˆå§‹åˆ†æè®°å½•å¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail=f"åˆ›å»ºåˆ†æè®°å½•å¤±è´¥: {str(e)}")

        # ç«‹å³å¯åŠ¨åå°åˆ†æä»»åŠ¡
        asyncio.create_task(process_page_analysis_task(session_id))

        logger.info(f"é¡µé¢åˆ†æä»»åŠ¡å·²åˆ›å»ºå¹¶å¯åŠ¨: {session_id}, æ–‡ä»¶æ•°é‡: {len(validated_files)}")

        return JSONResponse({
            "success": True,
            "message": "é¡µé¢åˆ†æä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°å¤„ç†",
            "data": {
                "session_id": session_id,
                "status": "processing",
                "uploaded_files": [f["filename"] for f in validated_files],
                "analysis_started": True,
                "sse_endpoint": f"/api/v1/web/page-analysis/stream/{session_id}",
                "status_endpoint": f"/api/v1/web/page-analysis/status/{session_id}",
                "files_info": [
                    {
                        "filename": f["filename"],
                        "size": f["size"]
                    } for f in validated_files
                ]
            }
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ›å»ºé¡µé¢åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºåˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/stream/{session_id}")
async def stream_page_analysis(
    session_id: str,
    request: Request,
    background_tasks: BackgroundTasks
):
    """
    é¡µé¢åˆ†æSSEæµå¼ç«¯ç‚¹

    Args:
        session_id: ä¼šè¯ID
        request: HTTPè¯·æ±‚å¯¹è±¡
        background_tasks: åå°ä»»åŠ¡ç®¡ç†å™¨

    Returns:
        EventSourceResponse: SSEå“åº”æµ
    """
    # éªŒè¯ä¼šè¯æ˜¯å¦å­˜åœ¨
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

    logger.info(f"å¼€å§‹é¡µé¢åˆ†æSSEæµ: {session_id}")

    # ç¡®ä¿æ¶ˆæ¯é˜Ÿåˆ—å­˜åœ¨
    if session_id not in message_queues:
        message_queue = asyncio.Queue()
        message_queues[session_id] = message_queue
        logger.info(f"åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—: {session_id}")
    else:
        message_queue = message_queues[session_id]
        logger.info(f"ä½¿ç”¨ç°æœ‰æ¶ˆæ¯é˜Ÿåˆ—: {session_id}")

    # è®¾ç½®ä¼šè¯è¶…æ—¶æ¸…ç†
    background_tasks.add_task(cleanup_session, session_id)

    # è¿”å›SSEå“åº”
    response = EventSourceResponse(
        page_analysis_event_generator(session_id, request),
        media_type="text/event-stream"
    )

    # æ·»åŠ å¿…è¦çš„å“åº”å¤´
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Connection"] = "keep-alive"
    response.headers["X-Accel-Buffering"] = "no"  # ç¦ç”¨Nginxç¼“å†²

    return response


async def page_analysis_event_generator(session_id: str, request: Request):
    """ç”Ÿæˆé¡µé¢åˆ†æSSEäº‹ä»¶æµ"""
    logger.info(f"å¼€å§‹ç”Ÿæˆé¡µé¢åˆ†æäº‹ä»¶æµ: {session_id}")

    # å‘é€ä¼šè¯åˆå§‹åŒ–äº‹ä»¶
    init_data = json.dumps({
        "session_id": session_id,
        "status": "connected",
        "service": "web_page_analysis"
    })
    yield f"event: session\nid: 0\ndata: {init_data}\n\n"

    # è·å–æ¶ˆæ¯é˜Ÿåˆ—
    message_queue = message_queues.get(session_id)
    if not message_queue:
        error_data = json.dumps({
            "error": "ä¼šè¯é˜Ÿåˆ—ä¸å­˜åœ¨"
        })
        yield f"event: error\nid: error-1\ndata: {error_data}\n\n"
        return

    # æ¶ˆæ¯IDè®¡æ•°å™¨
    message_id = 1

    try:
        # æŒç»­ä»é˜Ÿåˆ—è·å–æ¶ˆæ¯å¹¶å‘é€
        while True:
            # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€è¿æ¥
            if await request.is_disconnected():
                logger.info(f"å®¢æˆ·ç«¯æ–­å¼€è¿æ¥: {session_id}")
                break

            # å°è¯•ä»é˜Ÿåˆ—è·å–æ¶ˆæ¯ï¼ˆéé˜»å¡ï¼‰
            try:
                # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œç¡®ä¿æ›´é¢‘ç¹åœ°æ£€æŸ¥è¿æ¥çŠ¶æ€
                message = await asyncio.wait_for(message_queue.get(), timeout=0.5)

                # æ›´æ–°ä¼šè¯æœ€åæ´»åŠ¨æ—¶é—´
                if session_id in active_sessions:
                    active_sessions[session_id]["last_activity"] = datetime.now().isoformat()

                # ç¡®å®šäº‹ä»¶ç±»å‹
                event_type = message.type

                # å°†æ¶ˆæ¯è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                message_json = message.model_dump_json()

                logger.debug(f"å‘é€äº‹ä»¶: id={message_id}, type={event_type}, region={message.region}")

                # ä½¿ç”¨æ­£ç¡®çš„SSEæ ¼å¼å‘é€æ¶ˆæ¯
                yield f"event: {event_type}\nid: {message_id}\ndata: {message_json}\n\n"
                message_id += 1

                # å¦‚æœæ˜¯æœ€ç»ˆæ¶ˆæ¯ï¼Œå‘é€å®Œæˆäº‹ä»¶å¹¶å‡†å¤‡å…³é—­è¿æ¥
                if message.is_final:
                    logger.info(f"æ”¶åˆ°æœ€ç»ˆæ¶ˆæ¯ï¼Œå‡†å¤‡å…³é—­è¿æ¥: {session_id}")

                    # å‘é€æ˜ç¡®çš„å®Œæˆäº‹ä»¶
                    final_data = json.dumps({
                        "type": "final_result",
                        "source": "ç³»ç»Ÿ",
                        "content": "é¡µé¢åˆ†æå®Œæˆï¼",
                        "region": "process",
                        "is_final": True,
                        "result": message.result,
                        "timestamp": datetime.now().isoformat(),
                        "message_id": f"final-{session_id}",
                        "platform": "web"
                    })
                    yield f"event: final_result\nid: {message_id}\ndata: {final_data}\n\n"
                    message_id += 1

                    # çŸ­æš‚å»¶è¿Ÿåé€€å‡ºå¾ªç¯
                    await asyncio.sleep(0.5)
                    break

            except asyncio.TimeoutError:
                # å‘é€ä¿æŒè¿æ¥çš„æ¶ˆæ¯
                ping_data = json.dumps({"timestamp": datetime.now().isoformat()})
                yield f"event: ping\nid: ping-{message_id}\ndata: {ping_data}\n\n"
                message_id += 1
                continue

    except Exception as e:
        logger.error(f"ç”Ÿæˆäº‹ä»¶æµæ—¶å‡ºé”™: {str(e)}")
        error_data = json.dumps({
            "error": f"ç”Ÿæˆäº‹ä»¶æµæ—¶å‡ºé”™: {str(e)}"
        })
        yield f"event: error\nid: error-{message_id}\ndata: {error_data}\n\n"

    # å‘é€å…³é—­äº‹ä»¶
    close_data = json.dumps({
        "message": "æµå·²å…³é—­"
    })
    logger.info(f"äº‹ä»¶æµç»“æŸ: {session_id}")
    yield f"event: close\nid: close-{message_id}\ndata: {close_data}\n\n"


async def process_page_analysis_task(session_id: str):
    """å¤„ç†é¡µé¢åˆ†æçš„åå°ä»»åŠ¡"""
    logger.info(f"å¼€å§‹æ‰§è¡Œé¡µé¢åˆ†æä»»åŠ¡: {session_id}")

    try:
        # è·å–æ¶ˆæ¯é˜Ÿåˆ—
        message_queue = message_queues.get(session_id)
        if not message_queue:
            logger.error(f"ä¼šè¯ {session_id} çš„æ¶ˆæ¯é˜Ÿåˆ—ä¸å­˜åœ¨")
            return

        # è·å–ä¼šè¯ä¿¡æ¯
        session_info = active_sessions.get(session_id)
        if not session_info:
            logger.error(f"ä¼šè¯ {session_id} ä¿¡æ¯ä¸å­˜åœ¨")
            return

        # ä¼šè¯çŠ¶æ€å·²åœ¨ä¸Šä¼ æ—¶è®¾ç½®ä¸ºprocessing

        # å‘é€å¼€å§‹æ¶ˆæ¯
        message = StreamMessage(
            message_id=f"system-{uuid.uuid4()}",
            type="message",
            source="ç³»ç»Ÿ",
            content="ğŸš€ å¼€å§‹é¡µé¢åˆ†ææµç¨‹...",
            region="process",
            platform="web",
            is_final=False,
        )
        await message_queue.put(message)


        # è®¾ç½®æ¶ˆæ¯å›è°ƒå‡½æ•°
        async def message_callback(ctx: ClosureContext, message: StreamMessage, message_ctx: MessageContext) -> None:
            try:
                # è·å–å½“å‰é˜Ÿåˆ—ï¼ˆç¡®ä¿ä½¿ç”¨æœ€æ–°çš„é˜Ÿåˆ—å¼•ç”¨ï¼‰
                current_queue = message_queues.get(session_id)
                if current_queue:
                    await current_queue.put(message)
                else:
                    logger.error(f"æ¶ˆæ¯å›è°ƒï¼šä¼šè¯ {session_id} çš„é˜Ÿåˆ—ä¸å­˜åœ¨")

            except Exception as e:
                logger.error(f"æ¶ˆæ¯å›è°ƒå¤„ç†é”™è¯¯: {str(e)}")

        # åˆ›å»ºå“åº”æ”¶é›†å™¨
        from app.core.agents import StreamResponseCollector
        from app.core.types import AgentPlatform
        collector = StreamResponseCollector(platform=AgentPlatform.WEB)
        collector.set_callback(message_callback)

        # è·å–Webç¼–æ’å™¨
        from app.services.web.orchestrator_service import get_web_orchestrator
        orchestrator = get_web_orchestrator(collector=collector)

        # è·å–é¡µé¢ä¿¡æ¯
        page_info = session_info["page_info"]
        files = session_info["files"]

        # å¤„ç†æ¯ä¸ªä¸Šä¼ çš„æ–‡ä»¶
        for i, file_info in enumerate(files):
            try:
                # æ›´æ–°è¿›åº¦
                progress = int((i / len(files)) * 100)
                active_sessions[session_id]["progress"] = progress
                active_sessions[session_id]["processed_files"] = i
                active_sessions[session_id]["last_activity"] = datetime.now().isoformat()

                # å‘é€å¤„ç†è¿›åº¦æ¶ˆæ¯
                progress_message = StreamMessage(
                    message_id=f"progress-{uuid.uuid4()}",
                    type="message",
                    source="ç³»ç»Ÿ",
                    content=f"ğŸ“¸ æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(files)} ä¸ªé¡µé¢æˆªå›¾: {file_info['filename']} ({progress}%)",
                    region="process",
                    platform="web",
                    is_final=False,
                )
                await message_queue.put(progress_message)

                # ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆç‹¬ç«‹çš„åˆ†æIDï¼Œä½†ä¿æŒåŸå§‹session_id
                file_analysis_id = f"{session_id}_file_{i}"

                # ä½¿ç”¨ç¼–æ’å™¨æ‰§è¡Œé¡µé¢åˆ†æ
                await orchestrator.analyze_page_elements(
                    session_id=file_analysis_id,  # ä½¿ç”¨ç‹¬ç«‹çš„åˆ†æID
                    image_data=file_info["image_data"],
                    page_name=page_info.get("page_name", "") if page_info.get("page_name", "") else "",
                    page_description=page_info.get("description", "") if page_info.get("description", "") else "",
                    page_url=page_info.get("page_url", "")
                )

                # æ›´æ–°å®Œæˆçš„æ–‡ä»¶æ•°
                active_sessions[session_id]["processed_files"] = i + 1

            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {file_info['filename']} å¤±è´¥: {str(e)}")
                # å‘é€é”™è¯¯æ¶ˆæ¯ä½†ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
                error_message = StreamMessage(
                    message_id=f"error-{uuid.uuid4()}",
                    type="message",
                    source="ç³»ç»Ÿ",
                    content=f"âŒ æ–‡ä»¶ {file_info['filename']} åˆ†æå¤±è´¥: {str(e)}",
                    region="process",
                    platform="web",
                    is_final=False,
                )
                await message_queue.put(error_message)

        # å‘é€æœ€ç»ˆç»“æœ
        final_message = StreamMessage(
            message_id=f"final-{uuid.uuid4()}",
            type="final_result",
            source="ç³»ç»Ÿ",
            content="âœ… é¡µé¢åˆ†ææµç¨‹å®Œæˆï¼Œåˆ†æç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“",
            region="process",
            platform="web",
            is_final=True,
        )
        await message_queue.put(final_message)

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        active_sessions[session_id]["status"] = "completed"
        active_sessions[session_id]["progress"] = 100
        active_sessions[session_id]["processed_files"] = len(files)
        active_sessions[session_id]["completed_at"] = datetime.now().isoformat()
        active_sessions[session_id]["last_activity"] = datetime.now().isoformat()

        logger.info(f"é¡µé¢åˆ†æä»»åŠ¡å·²å®Œæˆ: {session_id}")

    except Exception as e:
        logger.error(f"é¡µé¢åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")

        # å‘é€é”™è¯¯æ¶ˆæ¯
        try:
            error_message = StreamMessage(
                message_id=f"error-{uuid.uuid4()}",
                type="error",
                source="ç³»ç»Ÿ",
                content=f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}",
                region="process",
                platform="web",
                is_final=True
            )

            message_queue = message_queues.get(session_id)
            if message_queue:
                await message_queue.put(error_message)

        except Exception as send_error:
            logger.error(f"å‘é€é”™è¯¯æ¶ˆæ¯å¤±è´¥: {str(send_error)}")

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        if session_id in active_sessions:
            active_sessions[session_id]["status"] = "error"
            active_sessions[session_id]["error"] = str(e)
            active_sessions[session_id]["error_at"] = datetime.now().isoformat()





@router.get("/sessions")
async def list_sessions():
    """åˆ—å‡ºæ‰€æœ‰æ´»åŠ¨ä¼šè¯"""
    return JSONResponse({
        "sessions": active_sessions,
        "total": len(active_sessions)
    })


@router.get("/status/{session_id}")
async def get_analysis_status(session_id: str):
    """è·å–åˆ†æçŠ¶æ€"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

    session_info = active_sessions[session_id]

    return JSONResponse({
        "success": True,
        "data": {
            "session_id": session_id,
            "status": session_info["status"],
            "progress": session_info.get("progress", 0),
            "total_files": session_info.get("total_files", 0),
            "processed_files": session_info.get("processed_files", 0),
            "created_at": session_info["created_at"],
            "last_activity": session_info["last_activity"],
            "error": session_info.get("error"),
            "completed_at": session_info.get("completed_at"),
            "page_info": session_info["page_info"]
        }
    })

@router.get("/sessions/{session_id}")
async def get_session(session_id: str):
    """è·å–æŒ‡å®šä¼šè¯çš„ä¿¡æ¯"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

    return JSONResponse(active_sessions[session_id])


@router.delete("/sessions/{session_id}")
async def delete_session(session_id: str):
    """åˆ é™¤æŒ‡å®šä¼šè¯"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

    # åˆ é™¤ä¼šè¯èµ„æº
    active_sessions.pop(session_id, None)
    message_queues.pop(session_id, None)

    return JSONResponse({
        "status": "success",
        "message": f"ä¼šè¯ {session_id} å·²åˆ é™¤"
    })


@router.get("/pages")
async def get_page_list(
    page: int = 1,
    page_size: int = 20,
    search: Optional[str] = None,
    status: Optional[str] = None,
    session: AsyncSession = Depends(get_db_session)
):
    """è·å–é¡µé¢åˆ—è¡¨"""
    try:
        logger.info(f"å¼€å§‹è·å–é¡µé¢åˆ—è¡¨ï¼Œé¡µç : {page}, é¡µé¢å¤§å°: {page_size}")

        repo = PageAnalysisRepository()

        if search:
            logger.info(f"æœç´¢é¡µé¢ï¼Œå…³é”®è¯: {search}")
            pages = await repo.search_by_page_name(session, search, limit=page_size)
        else:
            # è·å–æ‰€æœ‰é¡µé¢
            logger.info("è·å–æ‰€æœ‰é¡µé¢")
            from sqlalchemy import select, desc
            from app.database.models.page_analysis import PageAnalysisResult

            query = select(PageAnalysisResult).order_by(desc(PageAnalysisResult.created_at))
            result = await session.execute(query.limit(page_size).offset((page - 1) * page_size))
            pages = result.scalars().all()

        logger.info(f"æŸ¥è¯¢åˆ° {len(pages)} æ¡é¡µé¢è®°å½•")

        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        page_data = []
        for page_obj in pages:
            try:
                logger.debug(f"å¤„ç†é¡µé¢: {page_obj.id}")

                # ä½¿ç”¨æ”¹è¿›åçš„to_dictæ–¹æ³•
                page_dict = page_obj.to_dict()

                # æ·»åŠ åˆ†æçŠ¶æ€ï¼ˆåŸºäºç½®ä¿¡åº¦å’Œå…ƒç´ æ•°é‡æ¨æ–­ï¼‰
                elements_count = page_dict.get('elements_count', 0) or 0
                confidence_score = page_dict.get('confidence_score', 0) or 0

                if elements_count > 0 and confidence_score > 0:
                    page_dict['analysis_status'] = 'completed'
                elif elements_count == 0:
                    page_dict['analysis_status'] = 'pending'
                else:
                    page_dict['analysis_status'] = 'analyzing'

                page_data.append(page_dict)
                logger.debug(f"é¡µé¢ {page_obj.id} å¤„ç†æˆåŠŸ")

            except Exception as page_error:
                logger.error(f"å¤„ç†é¡µé¢ {page_obj.id} æ—¶å‡ºé”™: {page_error}")
                import traceback
                logger.error(f"é¡µé¢å¤„ç†è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

                # åˆ›å»ºä¸€ä¸ªæœ€å°çš„å®‰å…¨å­—å…¸
                safe_dict = {
                    "id": str(page_obj.id) if hasattr(page_obj, 'id') and page_obj.id else None,
                    "page_name": str(page_obj.page_name) if hasattr(page_obj, 'page_name') and page_obj.page_name else "æœªçŸ¥é¡µé¢",
                    "analysis_status": "error",
                    "error": "æ•°æ®å¤„ç†é”™è¯¯",
                    "confidence_score": 0.0,
                    "elements_count": 0,
                    "created_at": None,
                    "updated_at": None
                }
                page_data.append(safe_dict)

        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "success": True,
            "data": page_data,
            "total": len(page_data),
            "page": page,
            "page_size": page_size
        }

        logger.info(f"æˆåŠŸè·å–é¡µé¢åˆ—è¡¨ï¼Œå…± {len(page_data)} æ¡è®°å½•")
        return response_data

    except Exception as e:
        logger.error(f"è·å–é¡µé¢åˆ—è¡¨å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è·å–é¡µé¢åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/{page_id}")
async def get_page_analysis_detail(
    page_id: str,
    session: AsyncSession = Depends(get_db_session)
):
    """è·å–é¡µé¢åˆ†æè¯¦æƒ…"""
    try:
        from sqlalchemy import select
        from app.database.models.page_analysis import PageAnalysisResult, PageElement

        # è·å–é¡µé¢åˆ†æè®°å½•
        stmt = select(PageAnalysisResult).where(PageAnalysisResult.id == page_id)
        result = await session.execute(stmt)
        page_analysis = result.scalar_one_or_none()

        if not page_analysis:
            raise HTTPException(status_code=404, detail="é¡µé¢åˆ†æè®°å½•ä¸å­˜åœ¨")

        # è·å–é¡µé¢å…ƒç´ 
        elements_stmt = select(PageElement).where(PageElement.page_analysis_id == page_id)
        elements_result = await session.execute(elements_stmt)
        elements = elements_result.scalars().all()

        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "id": page_analysis.id,
            "session_id": page_analysis.session_id,
            "analysis_id": page_analysis.analysis_id,
            "page_name": page_analysis.page_name,
            "page_url": page_analysis.page_url,
            "page_type": page_analysis.page_type,
            "page_description": page_analysis.page_description,
            "analysis_summary": page_analysis.analysis_summary,
            "confidence_score": float(page_analysis.confidence_score) if page_analysis.confidence_score else 0.0,
            "raw_analysis_json": page_analysis.raw_analysis_json,
            "parsed_ui_elements": page_analysis.parsed_ui_elements,
            "analysis_metadata": page_analysis.analysis_metadata,
            "elements_count": page_analysis.elements_count,
            "processing_time": float(page_analysis.processing_time) if page_analysis.processing_time else 0.0,
            "created_at": page_analysis.created_at.isoformat() if page_analysis.created_at else None,
            "updated_at": page_analysis.updated_at.isoformat() if page_analysis.updated_at else None,
            "analysis_status": "completed",  # é»˜è®¤çŠ¶æ€ï¼Œå› ä¸ºèƒ½æŸ¥è¯¢åˆ°è¯´æ˜å·²å®Œæˆ
            "project_id": page_analysis.project_id,
            "elements": [
                {
                    "id": element.id,
                    "page_analysis_id": element.page_analysis_id,
                    "element_name": element.element_name,
                    "element_type": element.element_type,
                    "element_description": element.element_description,
                    "element_data": element.element_data,
                    "confidence_score": float(element.confidence_score) if element.confidence_score else 0.0,
                    "is_testable": element.is_testable,
                    "created_at": element.created_at.isoformat() if element.created_at else None,
                    "updated_at": element.updated_at.isoformat() if element.updated_at else None
                }
                for element in elements
            ]
        }

        return JSONResponse({
            "success": True,
            "data": response_data
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–é¡µé¢åˆ†æè¯¦æƒ…å¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è·å–é¡µé¢åˆ†æè¯¦æƒ…å¤±è´¥: {str(e)}")


@router.get("/pages/{page_id}/elements")
async def get_page_elements(
    page_id: str,
    session: AsyncSession = Depends(get_db_session)
):
    """è·å–é¡µé¢å…ƒç´ åˆ—è¡¨"""
    try:
        element_repo = PageElementRepository()
        elements = await element_repo.get_by_analysis_id(session, page_id)

        element_data = [element.to_dict() for element in elements]

        return JSONResponse({
            "success": True,
            "data": element_data,
            "total": len(element_data)
        })

    except Exception as e:
        logger.error(f"è·å–é¡µé¢å…ƒç´ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–é¡µé¢å…ƒç´ å¤±è´¥")


@router.delete("/pages/{page_id}")
async def delete_page(
    page_id: str,
    session: AsyncSession = Depends(get_db_session)
):
    """åˆ é™¤é¡µé¢"""
    try:
        repo = PageAnalysisRepository()
        page = await repo.get_by_id(session, page_id)

        if not page:
            raise HTTPException(status_code=404, detail="é¡µé¢ä¸å­˜åœ¨")

        # åˆ é™¤é¡µé¢è®°å½•ï¼ˆçº§è”åˆ é™¤å…ƒç´ ï¼‰
        await session.delete(page)
        await session.commit()

        return JSONResponse({
            "success": True,
            "message": "é¡µé¢åˆ é™¤æˆåŠŸ"
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤é¡µé¢å¤±è´¥: {e}")
        await session.rollback()
        raise HTTPException(status_code=500, detail="åˆ é™¤é¡µé¢å¤±è´¥")


# ==================== çŸ¥è¯†åº“ç›¸å…³æ¥å£ ====================

@router.get("/knowledge-base/summary")
async def get_knowledge_base_summary(
    session: AsyncSession = Depends(get_db_session)
):
    """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from sqlalchemy import select, func
        from app.database.models.page_analysis import PageAnalysisResult

        # è·å–æ€»é¡µé¢æ•°
        total_pages_query = select(func.count(PageAnalysisResult.id))
        total_pages_result = await session.execute(total_pages_query)
        total_pages = total_pages_result.scalar() or 0

        # è·å–é¡µé¢ç±»å‹ç»Ÿè®¡
        page_types_query = select(
            PageAnalysisResult.page_type,
            func.count(PageAnalysisResult.id).label('count')
        ).where(
            PageAnalysisResult.page_type.isnot(None)
        ).group_by(PageAnalysisResult.page_type).order_by(func.count(PageAnalysisResult.id).desc())

        page_types_result = await session.execute(page_types_query)
        page_types = [
            {"type": row.page_type, "count": row.count}
            for row in page_types_result.fetchall()
        ]

        summary = {
            "total_pages": total_pages,
            "page_types": page_types
        }

        return JSONResponse({
            "success": True,
            "data": summary
        })
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.get("/knowledge-base/search")
async def search_knowledge_base(
    query: str,
    page_type: Optional[str] = None,
    limit: int = 20,
    session: AsyncSession = Depends(get_db_session)
):
    """æœç´¢çŸ¥è¯†åº“"""
    try:
        from sqlalchemy import select, or_
        from app.database.models.page_analysis import PageAnalysisResult

        # æ„å»ºæŸ¥è¯¢
        stmt = select(PageAnalysisResult)

        search_conditions = []
        if query:
            # åœ¨é¡µé¢åç§°ã€æè¿°ä¸­æœç´¢
            search_conditions.append(
                or_(
                    PageAnalysisResult.page_name.ilike(f'%{query}%'),
                    PageAnalysisResult.page_description.ilike(f'%{query}%'),
                    PageAnalysisResult.analysis_summary.ilike(f'%{query}%')
                )
            )

        if page_type:
            search_conditions.append(
                PageAnalysisResult.page_type == page_type
            )

        if search_conditions:
            stmt = stmt.where(or_(*search_conditions))

        # æŒ‰ç½®ä¿¡åº¦å’Œåˆ›å»ºæ—¶é—´æ’åº
        stmt = stmt.order_by(
            PageAnalysisResult.confidence_score.desc(),
            PageAnalysisResult.created_at.desc()
        ).limit(limit)

        result = await session.execute(stmt)
        results = result.scalars().all()

        return JSONResponse({
            "success": True,
            "data": {
                "results": [result.to_dict() for result in results],
                "total": len(results),
                "query": query,
                "page_type": page_type
            }
        })
    except Exception as e:
        logger.error(f"æœç´¢çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æœç´¢çŸ¥è¯†åº“å¤±è´¥: {str(e)}")


@router.post("/knowledge-base/search-by-keywords")
async def search_by_keywords(
    request: dict,
    session: AsyncSession = Depends(get_db_session)
):
    """æ ¹æ®å…³é”®è¯æœç´¢çŸ¥è¯†åº“"""
    try:
        keywords = request.get("keywords", [])
        if not keywords:
            raise HTTPException(status_code=400, detail="å…³é”®è¯ä¸èƒ½ä¸ºç©º")

        from sqlalchemy import select, or_
        from app.database.models.page_analysis import PageAnalysisResult

        search_conditions = []
        for keyword in keywords:
            search_conditions.append(
                or_(
                    PageAnalysisResult.page_name.ilike(f'%{keyword}%'),
                    PageAnalysisResult.page_description.ilike(f'%{keyword}%'),
                    PageAnalysisResult.analysis_summary.ilike(f'%{keyword}%')
                )
            )

        stmt = select(PageAnalysisResult).where(
            or_(*search_conditions)
        ).order_by(
            PageAnalysisResult.confidence_score.desc(),
            PageAnalysisResult.created_at.desc()
        ).limit(20)

        result = await session.execute(stmt)
        results = result.scalars().all()

        return JSONResponse({
            "success": True,
            "data": {
                "results": [result.to_dict() for result in results],
                "total": len(results),
                "keywords": keywords
            }
        })
    except Exception as e:
        logger.error(f"å…³é”®è¯æœç´¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å…³é”®è¯æœç´¢å¤±è´¥: {str(e)}")


@router.get("/knowledge-base/ui-elements/{element_type}")
async def get_ui_elements_by_type(
    element_type: str,
    limit: int = 20,
    session: AsyncSession = Depends(get_db_session)
):
    """æ ¹æ®UIå…ƒç´ ç±»å‹è·å–ç¤ºä¾‹"""
    try:
        from sqlalchemy import select
        from app.database.models.page_analysis import PageElement

        stmt = select(PageElement).where(
            PageElement.element_type.ilike(f'%{element_type}%')
        ).limit(limit)

        result = await session.execute(stmt)
        elements = result.scalars().all()

        return JSONResponse({
            "success": True,
            "data": {
                "elements": [element.to_dict() for element in elements],
                "total": len(elements),
                "element_type": element_type
            }
        })
    except Exception as e:
        logger.error(f"è·å–UIå…ƒç´ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–UIå…ƒç´ å¤±è´¥: {str(e)}")


@router.get("/image/{image_path:path}")
async def get_analysis_image(image_path: str):
    """
    è·å–åˆ†æå›¾ç‰‡

    Args:
        image_path: å›¾ç‰‡è·¯å¾„

    Returns:
        å›¾ç‰‡æ–‡ä»¶
    """
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è·¯å¾„åœ¨uploadsç›®å½•å†…
        full_path = Path(image_path)
        if not str(full_path).startswith('uploads/images/'):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„å›¾ç‰‡è·¯å¾„")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not full_path.exists():
            raise HTTPException(status_code=404, detail="å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨")

        # è¿”å›å›¾ç‰‡æ–‡ä»¶
        return FileResponse(
            path=str(full_path),
            media_type="image/png",
            headers={"Cache-Control": "public, max-age=3600"}  # ç¼“å­˜1å°æ—¶
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å›¾ç‰‡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å›¾ç‰‡å¤±è´¥: {str(e)}")